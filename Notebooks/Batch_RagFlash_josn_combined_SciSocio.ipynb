{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:41:41.758619Z",
          "iopub.status.busy": "2025-01-03T03:41:41.758444Z",
          "iopub.status.idle": "2025-01-03T03:41:41.762162Z",
          "shell.execute_reply": "2025-01-03T03:41:41.761487Z",
          "shell.execute_reply.started": "2025-01-03T03:41:41.758601Z"
        },
        "id": "jepcotXpRPDU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['USER_AGENT'] = 'myagent'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-01-03T03:42:31.132378Z",
          "iopub.status.busy": "2025-01-03T03:42:31.132073Z",
          "iopub.status.idle": "2025-01-03T03:42:41.730460Z",
          "shell.execute_reply": "2025-01-03T03:42:41.729403Z",
          "shell.execute_reply.started": "2025-01-03T03:42:31.132351Z"
        },
        "id": "xQaCzL8SQ799",
        "outputId": "e93ce3bf-0d28-4634-da77-4b8f660a6c8f",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.37)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.10.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.8)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.15.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.45.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.67.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.37)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.10.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
            "Collecting json-repair\n",
            "  Using cached json_repair-0.39.1-py3-none-any.whl.metadata (11 kB)\n",
            "Using cached json_repair-0.39.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: json-repair\n",
            "Successfully installed json-repair-0.39.1\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.37)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.10.6)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.3)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.3.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.67.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.37->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.11-py3-none-any.whl (39 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.16-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.16 langchain-google-genai-2.0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "51525f5700dc4d06ba5aa3ae3686ed2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.37)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.8.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.26.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.10.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.6)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.28->langchain_experimental) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.1)\n",
            "Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.3.4\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community langchain-core\n",
        "!pip install PyMuPDF\n",
        "!pip install transformers sentence-transformers langchain\n",
        "!pip install chromadb\n",
        "!pip install langchain-huggingface\n",
        "!pip install json-repair\n",
        "!pip install -U langchain-google-genai  ## Using Chat Models\n",
        "!pip install langchain_experimental\n",
        "!pip install langchain\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:49:04.950769Z",
          "iopub.status.busy": "2025-01-03T03:49:04.950474Z",
          "iopub.status.idle": "2025-01-03T03:49:19.458795Z",
          "shell.execute_reply": "2025-01-03T03:49:19.458162Z",
          "shell.execute_reply.started": "2025-01-03T03:49:04.950747Z"
        },
        "id": "SLXCfs60Q2fm",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2845a1c-a5e6-40dc-9a5f-e721e50d9f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import fitz  # PyMuPDF for working with PDFs\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# LangChain modules\n",
        "from langchain.document_loaders import WebBaseLoader, UnstructuredPDFLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings, OllamaEmbeddings\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.prompts import ChatPromptTemplate, load_prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# LangChain community and retrievers\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# Hugging Face modules\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_google_genai import GoogleGenerativeAI,GoogleGenerativeAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:49:35.033923Z",
          "iopub.status.busy": "2025-01-03T03:49:35.033597Z",
          "iopub.status.idle": "2025-01-03T03:49:35.074205Z",
          "shell.execute_reply": "2025-01-03T03:49:35.073545Z",
          "shell.execute_reply.started": "2025-01-03T03:49:35.033899Z"
        },
        "id": "Tt7kGCXJe58H",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.graphs.networkx_graph import NetworkxEntityGraph\n",
        "from langchain.chains import GraphQAChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain import hub\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:49:40.607256Z",
          "iopub.status.busy": "2025-01-03T03:49:40.606856Z",
          "iopub.status.idle": "2025-01-03T03:49:40.611605Z",
          "shell.execute_reply": "2025-01-03T03:49:40.610872Z",
          "shell.execute_reply.started": "2025-01-03T03:49:40.607227Z"
        },
        "id": "rsdBTvj-QBWq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 1: Get arXiv Paper URLs\n",
        "def get_arxiv_paper_urls(query, max_results):\n",
        "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}\"\n",
        "    response = requests.get(url)\n",
        "    root = ET.fromstring(response.content)\n",
        "\n",
        "    paper_urls = []\n",
        "    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
        "        paper_url = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
        "        paper_urls.append(paper_url)\n",
        "\n",
        "    return paper_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:49:42.226490Z",
          "iopub.status.busy": "2025-01-03T03:49:42.226186Z",
          "iopub.status.idle": "2025-01-03T03:49:42.231565Z",
          "shell.execute_reply": "2025-01-03T03:49:42.230694Z",
          "shell.execute_reply.started": "2025-01-03T03:49:42.226464Z"
        },
        "id": "361Z8bKgQBWr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract Paper Metadata\n",
        "def extract_paper_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n",
        "    title = title_tag[\"content\"] if title_tag else \"Title not found\"\n",
        "\n",
        "    abstract_tag = soup.find(\"blockquote\", {\"class\": \"abstract\"})\n",
        "    abstract_text = abstract_tag.text.replace(\"Abstract: \", \"\").strip() if abstract_tag else \"Abstract not found\"\n",
        "\n",
        "    pdf_url = url.replace(\"abs\", \"pdf\") + \".pdf\"\n",
        "\n",
        "    return {\"title\": title, \"abstract\": abstract_text, \"pdf_url\": pdf_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-03T03:49:46.781890Z",
          "iopub.status.busy": "2025-01-03T03:49:46.781607Z",
          "iopub.status.idle": "2025-01-03T03:49:46.786628Z",
          "shell.execute_reply": "2025-01-03T03:49:46.785945Z",
          "shell.execute_reply.started": "2025-01-03T03:49:46.781868Z"
        },
        "id": "9mqBNXE4QBWr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Step 3: Download and Extract PDF\n",
        "def download_and_extract_pdf(pdf_url):\n",
        "    response = requests.get(pdf_url)\n",
        "    pdf_filename = pdf_url.split(\"/\")[-1]\n",
        "    with open(pdf_filename, \"wb\") as pdf_file:\n",
        "        pdf_file.write(response.content)\n",
        "\n",
        "    doc = fitz.open(pdf_filename)\n",
        "    full_text = \"\"\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc.load_page(page_num)\n",
        "        full_text += page.get_text(\"text\")\n",
        "    doc.close()\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i7Isfqo-jt-o"
      },
      "outputs": [],
      "source": [
        "def fetch_paper_urls(query, max_results):\n",
        "    \"\"\"Fetch arXiv paper URLs based on a query.\"\"\"\n",
        "    print(\"Fetching arXiv paper URLs...\")\n",
        "    urls = get_arxiv_paper_urls(query, max_results=max_results)\n",
        "    if not urls:\n",
        "        raise ValueError(\"No papers found for the given query.\")\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sslh1XBu0ixf"
      },
      "outputs": [],
      "source": [
        "def process_single_paper(url):\n",
        "    \"\"\"Process a single paper: extract metadata, download content, and return as a document.\"\"\"\n",
        "    try:\n",
        "        content = extract_paper_content(url)\n",
        "        print(f\"Processing: {content['title']}\")\n",
        "\n",
        "        # Download and extract PDF content\n",
        "        full_content = download_and_extract_pdf(content[\"pdf_url\"])\n",
        "        print(f\"Extracted content length: {len(full_content)} characters\")\n",
        "\n",
        "        # Return document format\n",
        "        return {\"title\": content[\"title\"], \"page_content\": full_content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sRtx3CVxjypj"
      },
      "outputs": [],
      "source": [
        "def process_papers(urls):\n",
        "    \"\"\"Process multiple papers and return a list of documents.\"\"\"\n",
        "    print(\"Processing papers...\")\n",
        "    docs = []\n",
        "    for url in urls:\n",
        "        doc = process_single_paper(url)\n",
        "        # Only append the document if it is not None\n",
        "        if doc is not None:\n",
        "            docs.append(doc)\n",
        "\n",
        "    # Raise an error if no valid documents were processed\n",
        "    if not docs:\n",
        "        raise ValueError(\"No valid content extracted from the papers.\")\n",
        "\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qnIGAzrEjln0"
      },
      "outputs": [],
      "source": [
        "def chunk_documents(documents, chunk_size=20000, chunk_overlap=2400):\n",
        "    \"\"\"Split documents into smaller chunks.\"\"\"\n",
        "    print(\"Splitting documents into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u7a2kVTcji54"
      },
      "outputs": [],
      "source": [
        "def initialize_embeddings(api_key, model=\"models/text-embedding-004\"):\n",
        "    \"\"\"Initialize the embedding model.\"\"\"\n",
        "    print(\"Initializing embeddings...\")\n",
        "    return GoogleGenerativeAIEmbeddings(model=model, google_api_key=api_key, task_type=\"retrieval_document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vi05fkfFjfLn"
      },
      "outputs": [],
      "source": [
        "def setup_FAISS_database(documents, embed):\n",
        "    \"\"\"Set up a database using the embeddings.\"\"\"\n",
        "    print(\"Setting up FAISS database...\")\n",
        "    return FAISS.from_documents(documents=documents, embedding=embed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from time import sleep\n",
        "\n",
        "def process_papers_and_create_database(query, max_results, batch_size=5):\n",
        "    \"\"\"\n",
        "    Orchestrate the pipeline to fetch, process, and prepare a FAISS database for academic papers in batches.\n",
        "    Also, combine all batch results into one consolidated JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        GOOGLE_API_KEY = \"\"  # Add your Google API key here\n",
        "\n",
        "        # Fetch all paper URLs\n",
        "        all_urls = fetch_paper_urls(query, max_results=max_results)\n",
        "        total_papers = len(all_urls)\n",
        "\n",
        "        # Calculate the number of batches\n",
        "        num_batches = math.ceil(total_papers / batch_size)\n",
        "        random.shuffle(all_urls)  # Shuffle to avoid repeating papers\n",
        "\n",
        "        all_results = []  # List to store all results from batches\n",
        "\n",
        "        for batch_num in range(num_batches):\n",
        "            start_index = batch_num * batch_size\n",
        "            current_batch_urls = all_urls[start_index:start_index + batch_size]\n",
        "\n",
        "            print(f\"Processing batch {batch_num + 1}/{num_batches}...\")\n",
        "\n",
        "            raw_docs = []\n",
        "            for i, url in enumerate(current_batch_urls):\n",
        "                try:\n",
        "                    # Process each paper individually and append to raw_docs if successful\n",
        "                    raw_doc = process_single_paper(url)\n",
        "                    if raw_doc:  # Only add successfully processed documents\n",
        "                        raw_docs.append(raw_doc)\n",
        "                    else:\n",
        "                        print(f\"Skipping paper {url} due to processing error.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing paper {url}: {e}. Skipping this paper.\")\n",
        "\n",
        "            if not raw_docs:  # If no documents were processed in the batch, skip the batch\n",
        "                print(f\"No papers processed in batch {batch_num + 1}, skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Convert raw_docs (dictionaries) to LangChain Document objects\n",
        "            docs = [\n",
        "                Document(page_content=doc.get(\"page_content\", \"\"), metadata={\"source\": current_batch_urls[i], \"title\": doc.get(\"title\", \"Untitled Document\")})\n",
        "                for i, doc in enumerate(raw_docs)\n",
        "            ]\n",
        "\n",
        "            # Chunk documents and set up database\n",
        "            chunks = chunk_documents(docs)\n",
        "            embed = initialize_embeddings(api_key=GOOGLE_API_KEY)\n",
        "            db = setup_FAISS_database(chunks, embed)\n",
        "\n",
        "            # Process queries and save the results\n",
        "            queries = [\"\"\"\n",
        "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
        "1. **Title** (from metadata)\n",
        "2. **Abstract** (if available)\n",
        "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
        "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
        "Provide clear and concise summaries for each paper.\n",
        "\"\"\"]\n",
        "\n",
        "            json_output_path = f\"rag_papers_results_batch_{batch_num + 1}.json\"\n",
        "            setup_rag_pipeline_and_process_queries_single(db, GOOGLE_API_KEY, queries, json_output_path)\n",
        "\n",
        "            # Load the JSON output for the current batch and append to all_results\n",
        "            with open(json_output_path, 'r') as f:\n",
        "                batch_results = json.load(f)\n",
        "                all_results.extend(batch_results)\n",
        "\n",
        "            print(f\"Batch {batch_num + 1} complete. Results saved to {json_output_path}.\")\n",
        "            sleep(30)  # Adding sleep to prevent hitting API rate limits if necessary\n",
        "\n",
        "        # After processing all batches, save the combined results to a single JSON file\n",
        "        combined_json_output_path = \"rag_papers_results_combined.json\"\n",
        "        with open(combined_json_output_path, 'w') as f:\n",
        "            json.dump(all_results, f, indent=4)\n",
        "\n",
        "        print(f\"All batches processed. Combined results saved to {combined_json_output_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "1cs--_u2o1K9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "from time import sleep\n",
        "\n",
        "def setup_rag_pipeline_and_process_queries_single(db, api_key, queries, json_output_path):\n",
        "    \"\"\"\n",
        "    Set up RAG pipeline, process multiple queries using the provided FAISS database, and save results to JSON.\n",
        "\n",
        "    Parameters:\n",
        "        db (FAISS database): The FAISS database containing processed papers.\n",
        "        api_key (str): API key for Google Generative AI.\n",
        "        queries (list): List of queries to process.\n",
        "        json_output_path (str): Path to save the JSON output.\n",
        "    \"\"\"\n",
        "    # Step 1: Check if db is provided, else raise error\n",
        "    if db is None:\n",
        "        print(\"Database is None. Exiting pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Configure MMR retriever\n",
        "    print(\"Setting up retriever...\")\n",
        "    retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 15, \"lambda_mult\": 0.5})\n",
        "\n",
        "    # Step 3: Initialize the LLM and RAG Chain\n",
        "    print(\"Setting up RAG chain...\")\n",
        "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key)\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "    # Step 4: Process each query and collect responses\n",
        "    results = []\n",
        "    for question in queries:\n",
        "        print(f\"Processing question: {question}\")\n",
        "        try:\n",
        "            response = rag_chain.invoke({\"query\": question})  # Use invoke instead of __call__\n",
        "            result_data = {\n",
        "                \"query\": question,\n",
        "                \"response\": response.get(\"result\", \"No result found.\"),\n",
        "                \"sources\": [\n",
        "                    doc.metadata.get(\"source\", \"Unknown source\") for doc in response.get(\"source_documents\", [])\n",
        "                ]\n",
        "            }\n",
        "            results.append(result_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question '{question}': {e}\")\n",
        "            results.append({\n",
        "                \"query\": question,\n",
        "                \"response\": f\"Error: {str(e)}\",\n",
        "                \"sources\": []\n",
        "            })\n",
        "\n",
        "    # Step 5: Save results to JSON\n",
        "    print(f\"Saving results to {json_output_path}...\")\n",
        "    try:\n",
        "        with open(json_output_path, \"w\") as f:\n",
        "            json.dump(results, f, indent=4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results to {json_output_path}: {e}\")\n",
        "\n",
        "    print(\"Processing complete!\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "dtxvPQu4BgfZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfadF3_6BRlq",
        "outputId": "4d758198-0b51-4df2-8e3f-f28061f5a2a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjr1nHoLMqQD",
        "outputId": "e55aba2f-cd06-4514-d695-29ce50b6d079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching arXiv paper URLs...\n",
            "Processing batch 1/40...\n",
            "Processing: Global to local impacts on atmospheric CO2 caused by COVID-19 lockdown\n",
            "Extracted content length: 50640 characters\n",
            "Processing: Planning low-carbon distributed power systems: Evaluating the role of energy storage\n",
            "Extracted content length: 77466 characters\n",
            "Processing: Carbon Intensity-Aware Adaptive Inference of DNNs\n",
            "Extracted content length: 11823 characters\n",
            "Processing: Costs and Benefits of the Paris Climate Targets\n",
            "Extracted content length: 36461 characters\n",
            "Processing: A Knowledge-driven Memetic Algorithm for the Energy-efficient Distributed Homogeneous Flow Shop Scheduling Problem\n",
            "Extracted content length: 62138 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_1.json...\n",
            "Processing complete!\n",
            "Batch 1 complete. Results saved to rag_papers_results_batch_1.json.\n",
            "Processing batch 2/40...\n",
            "Processing: Enabling Sustainable Clouds: The Case for Virtualizing the Energy System\n",
            "Extracted content length: 45973 characters\n",
            "Processing: Carbon financial system construction under the background of dual-carbon targets: current situation, problems and suggestions\n",
            "Extracted content length: 109617 characters\n",
            "Processing: Distinguishing Dynamic Phase Catalysis in Cu based nanostructures under Reverse Water Gas Shift Reaction\n",
            "Extracted content length: 12424 characters\n",
            "Processing: Two-stage Planning for Electricity-Gas Coupled Integrated Energy System with CCUS Considering Carbon Tax and Price Uncertainty\n",
            "Extracted content length: 47028 characters\n",
            "Processing: Impact of consumer preferences on decarbonization of transport sector in India\n",
            "Extracted content length: 72054 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_2.json...\n",
            "Processing complete!\n",
            "Batch 2 complete. Results saved to rag_papers_results_batch_2.json.\n",
            "Processing batch 3/40...\n",
            "Processing: Significant reduced traffic in Beijing failed to relieve haze pollution during the COVID-19 lockdown: implications for haze mitigation\n",
            "Extracted content length: 95146 characters\n",
            "Processing: Leveraging policy instruments and financial incentives to reduce embodied carbon in energy retrofits\n",
            "Extracted content length: 82989 characters\n",
            "Processing: Influence of microstructure and atomic-scale chemistry on iron ore reduction with hydrogen at 700{\\deg}C\n",
            "Extracted content length: 69883 characters\n",
            "Processing: Towards Carbon Transparency: A High-Resolution Carbon Emissions Database for China's Listed Companies\n",
            "Extracted content length: 60149 characters\n",
            "Processing: Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference\n",
            "Extracted content length: 76134 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_3.json...\n",
            "Processing complete!\n",
            "Batch 3 complete. Results saved to rag_papers_results_batch_3.json.\n",
            "Processing batch 4/40...\n",
            "Processing: On the means, costs, and system-level impacts of 24/7 carbon-free energy procurement\n",
            "Extracted content length: 75593 characters\n",
            "Processing: Electric vehicle charging during the day or at night: a perspective on carbon emissions\n",
            "Extracted content length: 25903 characters\n",
            "Processing: Effect of Methane Mitigation on Global Temperature under a Permafrost Feedback\n",
            "Extracted content length: 67481 characters\n",
            "Processing: Field emission theory beyond WKB - the full image problem\n",
            "Extracted content length: 37740 characters\n",
            "Processing: Adapting Datacenter Capacity for Greener Datacenters and Grid\n",
            "Extracted content length: 79453 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_4.json...\n",
            "Processing complete!\n",
            "Batch 4 complete. Results saved to rag_papers_results_batch_4.json.\n",
            "Processing batch 5/40...\n",
            "Processing: We must re-evaluate assumptions about carbon trading for effective climate change mitigation\n",
            "Extracted content length: 48782 characters\n",
            "Processing: Investigating climate tipping points under various emission reduction and carbon capture scenarios with a stochastic climate model\n",
            "Extracted content length: 51545 characters\n",
            "Processing: How Routing Strategies Impact Urban Emissions\n",
            "Extracted content length: 59141 characters\n",
            "Processing: A Market-Clearing-based Sensitivity Model for Locational Marginal and Average Carbon Emission\n",
            "Extracted content length: 20739 characters\n",
            "Processing: Atomic-scale effect of 2D {\\pi}-conjugated metal-organic frameworks as electrocatalysts for CO2 reduction reaction towards highly selective products\n",
            "Extracted content length: 35755 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_5.json...\n",
            "Processing complete!\n",
            "Batch 5 complete. Results saved to rag_papers_results_batch_5.json.\n",
            "Processing batch 6/40...\n",
            "Processing: Global carbon stocks and potential emissions due to mangrove deforestation from 2000 to 2012\n",
            "Extracted content length: 124801 characters\n",
            "Processing: Towards a Systematic Survey for Carbon Neutral Data Centers\n",
            "Extracted content length: 259149 characters\n",
            "Processing: A Framework for Carbon-aware Real-Time Workload Management in Clouds using Renewables-driven Cores\n",
            "Extracted content length: 66554 characters\n",
            "Processing: A Modified GHG Intensity Indicator: Toward a Sustainable Global Economy based on a Carbon Border Tax and Emissions Trading\n",
            "Extracted content length: 133917 characters\n",
            "Processing: Storage Control for Carbon Emission Reduction: Opportunities and Challenges\n",
            "Extracted content length: 21083 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_6.json...\n",
            "Processing complete!\n",
            "Batch 6 complete. Results saved to rag_papers_results_batch_6.json.\n",
            "Processing batch 7/40...\n",
            "Processing: A hypergraph model shows the carbon reduction potential of effective space use in housing\n",
            "Extracted content length: 63533 characters\n",
            "Processing: Oxygen Reduction Activity of Carbon Nitride Supported on Carbon Nanotubes\n",
            "Extracted content length: 16754 characters\n",
            "Processing: AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models\n",
            "Extracted content length: 57725 characters\n",
            "Processing: A Quantitative Investigation of CO2 Sequestration by Mineral Carbonation\n",
            "Extracted content length: 32119 characters\n",
            "Processing: ECO-CHIP: Estimation of Carbon Footprint of Chiplet-based Architectures for Sustainable VLSI\n",
            "Extracted content length: 85163 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_7.json...\n",
            "Processing complete!\n",
            "Batch 7 complete. Results saved to rag_papers_results_batch_7.json.\n",
            "Processing batch 8/40...\n",
            "Processing: Full Scaling Automation for Sustainable Development of Green Data Centers\n",
            "Extracted content length: 54672 characters\n",
            "Processing: Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at Scale\n",
            "Extracted content length: 152800 characters\n",
            "Processing: Assessing the optimal contributions of renewables and carbon capture and storage toward carbon neutrality by 2050\n",
            "Extracted content length: 56243 characters\n",
            "Processing: A zero-carbon, reliable and affordable energy future in Australia\n",
            "Extracted content length: 49369 characters\n",
            "Processing: An open-source tool to assess the carbon footprint of research\n",
            "Extracted content length: 31899 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_8.json...\n",
            "Processing complete!\n",
            "Batch 8 complete. Results saved to rag_papers_results_batch_8.json.\n",
            "Processing batch 9/40...\n",
            "Processing: Evaluation of Rail Decarbonization Alternatives: Framework and Application\n",
            "Extracted content length: 66526 characters\n",
            "Processing: Graphene-Supported Silver-Iron Carbon Nitride Derived from Thermal Decomposition of Silver Hexacyanoferrate as Effective Electrocatalyst for the Oxygen Reduction Reaction in Alkaline Media\n",
            "Extracted content length: 43841 characters\n",
            "Processing: Sensor Deployment and Link Analysis in Satellite IoT Systems for Wildfire Detection\n",
            "Extracted content length: 29842 characters\n",
            "Processing: Spatial-temporal evolution characteristics and driving factors of carbon emission prediction in China-research on ARIMA-BP neural network algorithm\n",
            "Extracted content length: 69803 characters\n",
            "Processing: Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions\n",
            "Extracted content length: 43712 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_9.json...\n",
            "Processing complete!\n",
            "Batch 9 complete. Results saved to rag_papers_results_batch_9.json.\n",
            "Processing batch 10/40...\n",
            "Processing: Emission impossible: Balancing Environmental Concerns and Inflation\n",
            "Extracted content length: 74425 characters\n",
            "Processing: The effect of price-based demand response on carbon emissions in European electricity markets: The importance of adequate carbon prices\n",
            "Extracted content length: 128181 characters\n",
            "Processing: Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction\n",
            "Extracted content length: 56303 characters\n",
            "Processing: Low latency carbon budget analysis reveals a large decline of the land carbon sink in 2023\n",
            "Extracted content length: 54016 characters\n",
            "Processing: Carbon-aware Software Services\n",
            "Extracted content length: 45234 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_10.json...\n",
            "Processing complete!\n",
            "Batch 10 complete. Results saved to rag_papers_results_batch_10.json.\n",
            "Processing batch 11/40...\n",
            "Processing: The Environmental Potential of Hyper-Scale Data Centers: Using Locational Marginal CO$_2$ Emissions to Guide Geographical Load Shifting\n",
            "Extracted content length: 49564 characters\n",
            "Processing: Green steel at its crossroads: hybrid hydrogen-based reduction of iron ores\n",
            "Extracted content length: 52404 characters\n",
            "Processing: Paths to Scalable Carbon Neutrality for MIT\n",
            "Extracted content length: 137630 characters\n",
            "Processing: Untangling Carbon-free Energy Attribution and Carbon Intensity Estimation for Carbon-aware Computing\n",
            "Extracted content length: 51827 characters\n",
            "Processing: Inflation Reduction Act impacts on the economics of clean hydrogen and liquid fuels\n",
            "Extracted content length: 110779 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_11.json...\n",
            "Processing complete!\n",
            "Batch 11 complete. Results saved to rag_papers_results_batch_11.json.\n",
            "Processing batch 12/40...\n",
            "Processing: Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference\n",
            "Extracted content length: 60207 characters\n",
            "Processing: Model Failure or Data Corruption? Exploring Inconsistencies in Building Energy Ratings with Self-Supervised Contrastive Learning\n",
            "Extracted content length: 14671 characters\n",
            "Processing: The emerging spectrum of flexible work locations: implications for travel demand and carbon emissions\n",
            "Extracted content length: 47844 characters\n",
            "Processing: Unleashing the full potential of the North Sea -- Identifying key energy infrastructure synergies for 2030 and 2040\n",
            "MuPDF error: syntax error: could not parse color space (583 0 R)\n",
            "\n",
            "Extracted content length: 120170 characters\n",
            "Processing: Unravelling of the chemistry and the performance in the oxygen reduction reaction of carbon nitride-supported bimetallic electrocatalysts through X-ray photoelectron spectroscopy\n",
            "Extracted content length: 58691 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_12.json...\n",
            "Processing complete!\n",
            "Batch 12 complete. Results saved to rag_papers_results_batch_12.json.\n",
            "Processing batch 13/40...\n",
            "Processing: Early decarbonisation of the European energy system pays off\n",
            "Extracted content length: 49242 characters\n",
            "Processing: Carbon Footprint Reduction for Sustainable Data Centers in Real-Time\n",
            "Extracted content length: 42135 characters\n",
            "Processing: Optimal Dataset Size for Recommender Systems: Evaluating Algorithms' Performance via Downsampling\n",
            "Extracted content length: 108973 characters\n",
            "Processing: Sustainable financing of permanent CO2 disposal through a Carbon Takeback Obligation\n",
            "Extracted content length: 34207 characters\n",
            "Processing: The Value of Recycling for Low-Carbon Energy Systems -- a Case Study of Germany's Energy Transition\n",
            "Extracted content length: 114269 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_13.json...\n",
            "Processing complete!\n",
            "Batch 13 complete. Results saved to rag_papers_results_batch_13.json.\n",
            "Processing batch 14/40...\n",
            "Processing: Polyacrylonitrile/Graphene Nanocomposite: Towards the Next Generation of Carbon Fibers\n",
            "Extracted content length: 54035 characters\n",
            "Processing: Embodied Carbon Accounting through Spatial-Temporal Embodied Carbon Models\n",
            "Extracted content length: 38872 characters\n",
            "Processing: Carbon Market Simulation with Adaptive Mechanism Design\n",
            "Extracted content length: 46788 characters\n",
            "Processing: Low-carbon optimal dispatch of integrated energy system considering demand response under the tiered carbon trading mechanism\n",
            "Extracted content length: 26326 characters\n",
            "Processing: Leveraging AI-derived Data for Carbon Accounting: Information Extraction from Alternative Sources\n",
            "Extracted content length: 22770 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_14.json...\n",
            "Processing complete!\n",
            "Batch 14 complete. Results saved to rag_papers_results_batch_14.json.\n",
            "Processing batch 15/40...\n",
            "Processing: Regional Impacts of COVID-19 on Carbon Dioxide Detected Worldwide from Space\n",
            "Extracted content length: 67091 characters\n",
            "Processing: Experimental implementation of an emission-aware prosumer with online flexibility quantification and provision\n",
            "Extracted content length: 57889 characters\n",
            "Processing: Strong reduction of exciton-phonon coupling in high crystalline quality single-wall carbon nanotubes: a new insight into broadening mechanisms and exciton localization\n",
            "Extracted content length: 24306 characters\n",
            "Processing: Carbon dioxide to carbon nanotube scale-up\n",
            "Extracted content length: 53980 characters\n",
            "Processing: Sustainable Multi-Modal Transportation and Routing focusing on Costs and Carbon Emissions Reduction\n",
            "Extracted content length: 20780 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_15.json...\n",
            "Processing complete!\n",
            "Batch 15 complete. Results saved to rag_papers_results_batch_15.json.\n",
            "Processing batch 16/40...\n",
            "Processing: LEAD: Towards Learning-Based Equity-Aware Decarbonization in Ridesharing Platforms\n",
            "Extracted content length: 57667 characters\n",
            "Processing: Treehouse: A Case For Carbon-Aware Datacenter Software\n",
            "Extracted content length: 44982 characters\n",
            "Processing: Equilibrium Strategies of Carbon Emission Reduction in Agricultural Product Supply Chain under Carbon Sink Trading\n",
            "Extracted content length: 49054 characters\n",
            "Processing: Advancing Carbon Capture using AI: Design of permeable membrane and estimation of parameters for Carbon Capture using linear regression and membrane-based equations\n",
            "Extracted content length: 94318 characters\n",
            "Processing: A Comprehensive Approach to Carbon Dioxide Emission Analysis in High Human Development Index Countries using Statistical and Machine Learning Techniques\n",
            "Extracted content length: 104931 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_16.json...\n",
            "Processing complete!\n",
            "Batch 16 complete. Results saved to rag_papers_results_batch_16.json.\n",
            "Processing batch 17/40...\n",
            "Processing: Carbon-Aware Optimal Power Flow\n",
            "Extracted content length: 74943 characters\n",
            "Processing: Assessing long-term medical remanufacturing emissions with Life Cycle Analysis\n",
            "Extracted content length: 93432 characters\n",
            "Processing: The impact of economic policy uncertainties on the volatility of European carbon market\n",
            "Extracted content length: 63146 characters\n",
            "Processing: Uncertainty-Aware Decarbonization for Datacenters\n",
            "Extracted content length: 37115 characters\n",
            "Processing: A Carbon Dioxide Absorption System Driven by Water Quantity\n",
            "Extracted content length: 39242 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_17.json...\n",
            "Processing complete!\n",
            "Batch 17 complete. Results saved to rag_papers_results_batch_17.json.\n",
            "Processing batch 18/40...\n",
            "Processing: A Holistic Approach for Equity-aware Carbon Reduction of Ridesharing Platforms\n",
            "Extracted content length: 74998 characters\n",
            "Processing: Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive Services\n",
            "Extracted content length: 87632 characters\n",
            "Processing: Joint Trading and Scheduling among Coupled Carbon-Electricity-Heat-Gas Industrial Clusters\n",
            "Extracted content length: 58637 characters\n",
            "Processing: Macro carbon price prediction with support vector regression and Paris accord targets\n",
            "Extracted content length: 23045 characters\n",
            "Processing: Quantification of the Impact of GHG Emissions on Unit Commitment in Microgrids\n",
            "Extracted content length: 30859 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_18.json...\n",
            "Processing complete!\n",
            "Batch 18 complete. Results saved to rag_papers_results_batch_18.json.\n",
            "Processing batch 19/40...\n",
            "Processing: Achieving Dispatchability in Data Centers: Carbon and Cost-Aware Sizing of Energy Storage and Local Photovoltaic Generation\n",
            "Extracted content length: 64344 characters\n",
            "Processing: Applying endogenous learning models in energy system optimization\n",
            "Extracted content length: 77018 characters\n",
            "Processing: Low-Carbon Economic Dispatch of Bulk Power Systems Using Nash Bargaining Game\n",
            "Extracted content length: 25511 characters\n",
            "Processing: Upgrading Urban Water Storage System: Achieving Water Conservation, Power Generation, Carbon Reduction, and Water Quality Enhancement\n",
            "Extracted content length: 29416 characters\n",
            "Processing: FedGreen: Carbon-aware Federated Learning with Model Size Adaptation\n",
            "Extracted content length: 29186 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_19.json...\n",
            "Processing complete!\n",
            "Batch 19 complete. Results saved to rag_papers_results_batch_19.json.\n",
            "Processing batch 20/40...\n",
            "Processing: Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning\n",
            "Extracted content length: 113673 characters\n",
            "Processing: Well-to-Tank Carbon Intensity Variability of Fossil Marine Fuels: A Country-Level Assessment\n",
            "Extracted content length: 39626 characters\n",
            "Processing: Identifying Operation Equilibrium in Integrated Electricity, Natural Gas, and Carbon-Emission Markets\n",
            "Extracted content length: 56819 characters\n",
            "Processing: Contract Design for V2G Smart Energy Trading\n",
            "Error processing http://arxiv.org/abs/2309.09671v2: Failed to open file '2309.09671v2.pdf'.\n",
            "Skipping paper http://arxiv.org/abs/2309.09671v2 due to processing error.\n",
            "Processing: Contributions of Individual Generators to Nodal Carbon Emissions\n",
            "Extracted content length: 37542 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_20.json...\n",
            "Processing complete!\n",
            "Batch 20 complete. Results saved to rag_papers_results_batch_20.json.\n",
            "Processing batch 21/40...\n",
            "Processing: Engineering Carbon Credits Towards A Responsible FinTech Era: The Practices, Implications, and Future\n",
            "Extracted content length: 125640 characters\n",
            "Processing: Direct reduction of iron-ore with hydrogen in fluidized beds: A coarse-grained CFD-DEM-IBM study\n",
            "Extracted content length: 81157 characters\n",
            "Processing: Carbon-Aware End-to-End Data Movement\n",
            "Extracted content length: 39053 characters\n",
            "Processing: Integration of hydrothermal liquefaction and carbon capture and storage for the production of advanced liquid biofuels with negative CO2 emissions\n",
            "Extracted content length: 71091 characters\n",
            "Processing: The carbon footprint of astronomical observatories\n",
            "Extracted content length: 33549 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_21.json...\n",
            "Processing complete!\n",
            "Batch 21 complete. Results saved to rag_papers_results_batch_21.json.\n",
            "Processing batch 22/40...\n",
            "Processing: China's plug-in hybrid electric vehicle transition: an operational carbon perspective\n",
            "Extracted content length: 82821 characters\n",
            "Processing: Shifting burdens: How delayed decarbonisation of road transport affects other sectoral emission reductions\n",
            "Extracted content length: 72017 characters\n",
            "Processing: Profitable Emissions-Reducing Energy Storage\n",
            "Extracted content length: 60343 characters\n",
            "Processing: Sustainable steel through hydrogen plasma reduction of iron ore: process, kinetics, microstructure, chemistry\n",
            "Extracted content length: 77824 characters\n",
            "Processing: SolarEV City Concept for Paris: A promising idea?\n",
            "Extracted content length: 49336 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_22.json...\n",
            "Processing complete!\n",
            "Batch 22 complete. Results saved to rag_papers_results_batch_22.json.\n",
            "Processing batch 23/40...\n",
            "Processing: Optimal dynamic regulation of carbon emissions market: A variational approach\n",
            "Extracted content length: 73491 characters\n",
            "Processing: Carbon Containers: A System-level Facility for Managing Application-level Carbon Emissions\n",
            "Extracted content length: 80739 characters\n",
            "Processing: Efficient Strategies on Supply Chain Network Optimization for Industrial Carbon Emission Reduction\n",
            "Extracted content length: 42848 characters\n",
            "Processing: Assessing trade-offs among electrification and grid decarbonization in a clean energy transition: Application to New York State\n",
            "Extracted content length: 176156 characters\n",
            "Processing: What trade-off for astronomy between greenhouse gas emissions and the societal benefits? A sociological approach\n",
            "Extracted content length: 23405 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_23.json...\n",
            "Processing complete!\n",
            "Batch 23 complete. Results saved to rag_papers_results_batch_23.json.\n",
            "Processing batch 24/40...\n",
            "Processing: Experimental and numerical study on the effect of oxymethylene ether-3 (OME3) on soot particle formation\n",
            "Extracted content length: 60604 characters\n",
            "Processing: Observations and modelling of CO and [CI] in disks. First detections of [CI] and constraints on the carbon abundance\n",
            "Extracted content length: 70273 characters\n",
            "Processing: A Study of Cooling Time Reduction of Interferometric Cryogenic Gravitational Wave Detectors Using a High-Emissivity Coating\n",
            "Extracted content length: 20529 characters\n",
            "Processing: Maintaining Performance with Less Data\n",
            "Extracted content length: 45149 characters\n",
            "Processing: Reward-penalty Mechanism for Reverse Supply Chain Network with Asymmetric Information and Carbon Emission Constraints\n",
            "Extracted content length: 61882 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_24.json...\n",
            "Processing complete!\n",
            "Batch 24 complete. Results saved to rag_papers_results_batch_24.json.\n",
            "Processing batch 25/40...\n",
            "Processing: An Emissions Trading System to reach NDC targets in the Chilean electric sector\n",
            "Extracted content length: 61403 characters\n",
            "Processing: Are biofuel mandates cost-effective? -- an analysis of transport fuels and biomass usage to achieve emissions targets in the European energy system\n",
            "Extracted content length: 91170 characters\n",
            "Processing: The Sunk Carbon Fallacy: Rethinking Carbon Footprint Metrics for Effective Carbon-Aware Scheduling\n",
            "Extracted content length: 67080 characters\n",
            "Processing: GreenLLM: Disaggregating Large Language Model Serving on Heterogeneous GPUs for Lower Carbon Emissions\n",
            "Extracted content length: 64702 characters\n",
            "Processing: Impact of the Inflation Reduction Act and Carbon Capture on Transportation Electrification for a Net-Zero Western U.S. Grid\n",
            "Extracted content length: 58540 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_25.json...\n",
            "Processing complete!\n",
            "Batch 25 complete. Results saved to rag_papers_results_batch_25.json.\n",
            "Processing batch 26/40...\n",
            "Processing: B-ETS: A Trusted Blockchain-based Emissions Trading System for Vehicle-to-Vehicle Networks\n",
            "Extracted content length: 36408 characters\n",
            "Processing: Simulating the deep decarbonisation of residential heating for limiting global warming to 1.5C\n",
            "Extracted content length: 117654 characters\n",
            "Processing: Emissions and Energy Impacts of the Inflation Reduction Act\n",
            "Extracted content length: 107519 characters\n",
            "Processing: Nonlinear Decision Rule Approach for Real-Time Traffic Signal Control for Congestion and Emission Reductions\n",
            "Extracted content length: 72769 characters\n",
            "Processing: Carbon-Aware Computing in a Network of Data Centers: A Hierarchical Game-Theoretic Approach\n",
            "Extracted content length: 32656 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_26.json...\n",
            "Processing complete!\n",
            "Batch 26 complete. Results saved to rag_papers_results_batch_26.json.\n",
            "Processing batch 27/40...\n",
            "Processing: Equitable Network-Aware Decarbonization of Residential Heating at City Scale\n",
            "Extracted content length: 78108 characters\n",
            "Processing: A Microgrid Deployment Framework to Support Drayage Electrification\n",
            "Extracted content length: 115001 characters\n",
            "Processing: Fleet-Level Environmental Assessments for Feasibility of Aviation Emission Reduction Goals\n",
            "Extracted content length: 43242 characters\n",
            "Processing: The effect of inhomogeneous carbon prices on the cost-optimal design of a simplified European power system\n",
            "Extracted content length: 24159 characters\n",
            "Processing: Abominable greenhouse gas bookkeeping casts serious doubts on climate intentions of oil and gas companies\n",
            "Extracted content length: 38978 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_27.json...\n",
            "Processing complete!\n",
            "Batch 27 complete. Results saved to rag_papers_results_batch_27.json.\n",
            "Processing batch 28/40...\n",
            "Processing: Carbon Emission Reduction Effect of RMB Appreciation: Empirical Evidence from 283 Prefecture-Level Cities of China\n",
            "Extracted content length: 34468 characters\n",
            "Processing: Optimization of Hydrogen Blending in Natural Gas Networks for Carbon Emissions Reduction\n",
            "Extracted content length: 39759 characters\n",
            "Processing: Encoding Carbon Emission Flow in Energy Management: A Compact Constraint Learning Approach\n",
            "Extracted content length: 64309 characters\n",
            "Processing: Nonparametric approaches for analyzing carbon emission: from statistical and machine learning perspectives\n",
            "Extracted content length: 34598 characters\n",
            "Processing: Urban and non-urban contributions to the social cost of carbon\n",
            "Extracted content length: 72085 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_28.json...\n",
            "Processing complete!\n",
            "Batch 28 complete. Results saved to rag_papers_results_batch_28.json.\n",
            "Processing batch 29/40...\n",
            "Processing: Consequences of glacial cycles for magmatism and carbon transport at mid-ocean ridges\n",
            "Extracted content length: 81688 characters\n",
            "Processing: Electric-Gas Infrastructure Planning for Deep Decarbonization of Energy Systems\n",
            "Extracted content length: 122814 characters\n",
            "Processing: Distribution Locational Marginal Emission for Carbon Alleviation in Distribution Networks: Formulation, Calculation, and Implication\n",
            "Extracted content length: 48419 characters\n",
            "Processing: Measuring the Carbon Intensity of AI in Cloud Instances\n",
            "Extracted content length: 79214 characters\n",
            "Processing: Environmental Kuznets Curve & Effectiveness of International Policies: Evidence from Cross Country Carbon Emission Analysis\n",
            "Extracted content length: 68823 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_29.json...\n",
            "Processing complete!\n",
            "Batch 29 complete. Results saved to rag_papers_results_batch_29.json.\n",
            "Processing batch 30/40...\n",
            "Processing: Quantum Noise Measurement of a Carbon Nanotube Quantum Dot in the Kondo Regime\n",
            "Extracted content length: 28869 characters\n",
            "Processing: On the Promise and Pitfalls of Optimizing Embodied Carbon\n",
            "Extracted content length: 37094 characters\n",
            "Processing: A hybrid deep learning approach for purchasing strategy of carbon emission rights -- Based on Shanghai pilot market\n",
            "Extracted content length: 56011 characters\n",
            "Processing: Reconsidering the origin of the 21 micron feature: Oxides in carbon-rich PPNe?\n",
            "Extracted content length: 66328 characters\n",
            "Processing: Dynamic Incentive Allocation for City-scale Deep Decarbonization\n",
            "Extracted content length: 89250 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_30.json...\n",
            "Processing complete!\n",
            "Batch 30 complete. Results saved to rag_papers_results_batch_30.json.\n",
            "Processing batch 31/40...\n",
            "Processing: Emission-aware Energy Storage Scheduling for a Greener Grid\n",
            "Extracted content length: 62532 characters\n",
            "Processing: Near-Optimal Emission-Aware Online Ride Assignment Algorithm for Peak Demand Hours\n",
            "Extracted content length: 65315 characters\n",
            "Processing: Carbon-Aware Computing for Data Centers with Probabilistic Performance Guarantees\n",
            "Extracted content length: 54850 characters\n",
            "Processing: Differential Game Analysis for Cooperation Models in Automotive Supply Chain under Low-Carbon Emission Reduction Policies\n",
            "Extracted content length: 38684 characters\n",
            "Processing: Power system investment optimization to identify carbon neutrality scenarios for Italy\n",
            "Extracted content length: 77519 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_31.json...\n",
            "Processing complete!\n",
            "Batch 31 complete. Results saved to rag_papers_results_batch_31.json.\n",
            "Processing batch 32/40...\n",
            "Processing: Study of SEY degradation of amorphous carbon coatings\n",
            "Extracted content length: 17585 characters\n",
            "Processing: On the Limitations of Carbon-Aware Temporal and Spatial Workload Shifting in the Cloud\n",
            "Extracted content length: 91904 characters\n",
            "Processing: TRIZ Method for Urban Building Energy Optimization: GWO-SARIMA-LSTM Forecasting model\n",
            "Extracted content length: 82149 characters\n",
            "Processing: Carbon Dioxide Production Responsibility on the Basis of comparing in Situ and mean CO2 Atmosphere Concentration Data\n",
            "Extracted content length: 25201 characters\n",
            "Processing: Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN\n",
            "Extracted content length: 28601 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_32.json...\n",
            "Processing complete!\n",
            "Batch 32 complete. Results saved to rag_papers_results_batch_32.json.\n",
            "Processing batch 33/40...\n",
            "Processing: LACS: Learning-Augmented Algorithms for Carbon-Aware Resource Scaling with Uncertain Demand\n",
            "Extracted content length: 100778 characters\n",
            "Processing: Carbon-Neutralized Task Scheduling for Green Computing Networks\n",
            "Extracted content length: 30694 characters\n",
            "Processing: Enhancing Carbon Emission Reduction Strategies using OCO and ICOS data\n",
            "Extracted content length: 56279 characters\n",
            "Processing: CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web Services\n",
            "Extracted content length: 38628 characters\n",
            "Processing: Impacts of EPA Power Plant Emissions Regulations on the US Electricity Sector\n",
            "Extracted content length: 50448 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_33.json...\n",
            "Processing complete!\n",
            "Batch 33 complete. Results saved to rag_papers_results_batch_33.json.\n",
            "Processing batch 34/40...\n",
            "Processing: Decarbonizing OCP\n",
            "Extracted content length: 112699 characters\n",
            "Processing: Carbon Footprint in Indonesia Plantation Sector: GHG Calculation for Main Commodities\n",
            "Extracted content length: 55069 characters\n",
            "Processing: Computational Fluid Dynamics: its Carbon Footprint and Role in Carbon Emission Reduction\n",
            "Extracted content length: 58235 characters\n",
            "Processing: Carbon emissions and sustainability of launching 5G mobile networks in China\n",
            "Extracted content length: 184106 characters\n",
            "Processing: Congestion Reduction via Personalized Incentives\n",
            "Extracted content length: 76437 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_34.json...\n",
            "Processing complete!\n",
            "Batch 34 complete. Results saved to rag_papers_results_batch_34.json.\n",
            "Processing batch 35/40...\n",
            "Processing: Valorizing the carbon byproduct of methane pyrolysis in batteries\n",
            "Extracted content length: 84096 characters\n",
            "Processing: The Scope 4 Emission: Neutralized Carbon Emissions\n",
            "Extracted content length: 31101 characters\n",
            "Processing: Molecular screening effects on exciton-carrier interactions in suspend carbon nanotubes\n",
            "Extracted content length: 21572 characters\n",
            "Processing: The distribution dynamics of Carbon Dioxide Emission intensity across Chinese provinces: A weighted Approach\n",
            "Extracted content length: 52970 characters\n",
            "Processing: Grid-level impacts of renewable energy on thermal generation: efficiency, emissions and flexibility\n",
            "Extracted content length: 64471 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_35.json...\n",
            "Processing complete!\n",
            "Batch 35 complete. Results saved to rag_papers_results_batch_35.json.\n",
            "Processing batch 36/40...\n",
            "Processing: The Green Mirage: Impact of Location- and Market-based Carbon Intensity Estimation on Carbon Optimization Efficacy\n",
            "Extracted content length: 64873 characters\n",
            "Processing: Efficient Emission Reduction Through Dynamic Supply Mode Selection\n",
            "Extracted content length: 96369 characters\n",
            "Processing: Little to lose: the case for a robust European green hydrogen strategy\n",
            "Extracted content length: 99240 characters\n",
            "Processing: Graphene-Supported Au-Ni Carbon Nitride Electrocatalysts for the ORR in Alkaline Environment\n",
            "Extracted content length: 30011 characters\n",
            "Processing: Multi-Stage Expansion Planning for Decarbonizing Thermal Generation Supported Renewable Power Systems Using Hydrogen and Ammonia Storage\n",
            "Extracted content length: 53459 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_36.json...\n",
            "Processing complete!\n",
            "Batch 36 complete. Results saved to rag_papers_results_batch_36.json.\n",
            "Processing batch 37/40...\n",
            "Processing: Advancing Environmental Sustainability in Data Centers by Proposing Carbon Depreciation Models\n",
            "Extracted content length: 74252 characters\n",
            "Processing: Are EU low-carbon structural funds efficient in reducing emissions?\n",
            "Extracted content length: 91240 characters\n",
            "Processing: Forecasting fuel combustion-related CO$_2$ emissions by a novel continuous fractional nonlinear grey Bernoulli model with Grey Wolf Optimizer\n",
            "Extracted content length: 54311 characters\n",
            "Processing: Global and regional changes in carbon dioxide emissions: 1970-2019\n",
            "Extracted content length: 46868 characters\n",
            "Processing: ElectricityEmissions.jl: A Framework for the Comparison of Carbon Intensity Signals\n",
            "Extracted content length: 67067 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_37.json...\n",
            "Processing complete!\n",
            "Batch 37 complete. Results saved to rag_papers_results_batch_37.json.\n",
            "Processing batch 38/40...\n",
            "Processing: Carbon-Neutralized Joint User Association and Base Station Switching for Green Cellular Networks\n",
            "Extracted content length: 30618 characters\n",
            "Processing: Provincial allocation of China's commercial building operational carbon towards carbon neutrality\n",
            "Extracted content length: 65646 characters\n",
            "Processing: Historical Evolution of Global Inequality in Carbon Emissions and Footprints versus Redistributive Scenarios\n",
            "Extracted content length: 86989 characters\n",
            "Processing: Advanced Models for Hourly Marginal CO2 Emission Factor Estimation: A Synergy between Fundamental and Statistical Approaches\n",
            "Extracted content length: 122710 characters\n",
            "Processing: The carbon footprint of astronomical research infrastructures\n",
            "Extracted content length: 26894 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_38.json...\n",
            "Processing complete!\n",
            "Batch 38 complete. Results saved to rag_papers_results_batch_38.json.\n",
            "Processing batch 39/40...\n",
            "Processing: Generative AI for Low-Carbon Artificial Intelligence of Things with Large Language Models\n",
            "Extracted content length: 56459 characters\n",
            "Processing: CarbonCP: Carbon-Aware DNN Partitioning with Conformal Prediction for Sustainable Edge Intelligence\n",
            "Extracted content length: 52182 characters\n",
            "Processing: Greening the Grid: Electricity Market Clearing with Consumer-Based Carbon Cost\n",
            "Extracted content length: 52042 characters\n",
            "Processing: \"My Earth\" Astrophysics and Planets -- a serious game to build low carbon scenarios in the astronomy academic community\n",
            "Extracted content length: 10294 characters\n",
            "Processing: Clover: Toward Sustainable AI with Carbon-Aware Machine Learning Inference Service\n",
            "Extracted content length: 86891 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_39.json...\n",
            "Processing complete!\n",
            "Batch 39 complete. Results saved to rag_papers_results_batch_39.json.\n",
            "Processing batch 40/40...\n",
            "Processing: Firm-level supply chains to minimize unemployment and economic losses in rapid decarbonization scenarios\n",
            "Extracted content length: 88761 characters\n",
            "Processing: Economic Viability of the Energy-Water-Hydrogen Nexus for Power System Decarbonization\n",
            "Extracted content length: 28077 characters\n",
            "Processing: Development dilemma of ride-sharing: Revenue or social welfare?\n",
            "Extracted content length: 73520 characters\n",
            "Processing: Fe-carbon nitride 'Core-shell' electrocatalysts for the oxygen reduction reaction\n",
            "Extracted content length: 71785 characters\n",
            "Processing: Does Environmental Attention by Governments Promote Carbon Reductions\n",
            "Extracted content length: 51294 characters\n",
            "Splitting documents into chunks...\n",
            "Initializing embeddings...\n",
            "Setting up FAISS database...\n",
            "Setting up retriever...\n",
            "Setting up RAG chain...\n",
            "Processing question: \n",
            "Summarize the main idea and key results of each paper using the provided excerpts and metadata. Include:\n",
            "1. **Title** (from metadata)\n",
            "2. **Abstract** (if available)\n",
            "3. **Main idea and hypothesis**: It should clearly states the hypothesis which the paper is written based on it accordingly.\n",
            "4. **Summary of Results**: Key findings, conclusions, or implications.\n",
            "Provide clear and concise summaries for each paper.\n",
            "\n",
            "Saving results to rag_papers_results_batch_40.json...\n",
            "Processing complete!\n",
            "Batch 40 complete. Results saved to rag_papers_results_batch_40.json.\n",
            "All batches processed. Combined results saved to rag_papers_results_combined.json.\n"
          ]
        }
      ],
      "source": [
        "query = \"Carbon Emissions Reduction\"\n",
        "max_results = 200  # Number of papers to fetch\n",
        "process_papers_and_create_database(query=query, max_results=max_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define the source directory (e.g., '/content/')\n",
        "source_directory = '/content/'\n",
        "\n",
        "# Define the output zip file path (e.g., '/home/json_files.zip')\n",
        "output_zip_file = '/content/sample_data/json_files.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(output_zip_file, 'w') as zipf:\n",
        "    # Walk through the source directory\n",
        "    for root, dirs, files in os.walk(source_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):  # Only include JSON files\n",
        "                # Get the full file path\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add the file to the zip, preserving the directory structure\n",
        "                arcname = os.path.relpath(file_path, source_directory)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"All JSON files in '{source_directory}' have been zipped and saved to '{output_zip_file}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAMMez2k4KP8",
        "outputId": "7e9bb400-c02b-4357-f396-936721d60a6e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All JSON files in '/content/' have been zipped and saved to '/content/sample_data/json_files.zip'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}